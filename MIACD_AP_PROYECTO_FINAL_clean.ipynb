{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluación Comparativa de Modelos CNN en Imágenes Médicas (Kvasir-SEG)\n",
        "\n",
        "**Autor:** Carlos Eduardo Tiscareño Carreón\n",
        "\n",
        "**Fecha:** 18 de septiembre del 2025\n",
        "\n",
        "**Maestria en Ciencia de datos e inteligencia Artificial**\n",
        "\n",
        "**Materia:** Aprendizaje Profundo\n",
        "\n",
        "**Profesor:** Dr. Ricardo Abel Espinosa Loera\n",
        "\n",
        "\n",
        "## Resumen del Proyecto\n",
        "Este proyecto analiza y compara el desempeño de tres arquitecturas de Redes Neuronales Convolucionales (CNNs) pre-entrenadas para la tarea de clasificación multi-clase de imágenes de endoscopia del dataset Kvasir-v2. Los modelos evaluados son ResNet50, EfficientNet-B0 y DenseNet121.\n",
        "\n",
        "La evaluación se realiza en base a tres criterios clave:\n",
        "1.  **Precisión del Modelo:** Accuracy y F1-Score en el conjunto de prueba.\n",
        "2.  **Costo Computacional:** Tiempo de entrenamiento y número de parámetros.\n",
        "3.  **Interpretabilidad:** Visualización de las áreas de interés del modelo mediante Grad-CAM.\n",
        "\n",
        "El código está estructurado para ser completamente reproducible."
      ],
      "metadata": {
        "id": "yGH2wGuHEVX9"
      },
      "id": "yGH2wGuHEVX9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RHgc64RED80"
      },
      "outputs": [],
      "source": [
        "# ===================================================================\n",
        "# 1. CONFIGURACIÓN DEL ENTORNO\n",
        "# ===================================================================\n",
        "\n",
        "# 1.1. Montar Google Drive para persistencia de datos\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 1.2. Importación de Librerías Principales\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import pickle\n",
        "# 1.3. Definición de Parámetros y Constantes del Proyecto\n",
        "# Esta celda centraliza todas las configuraciones para facilitar ajustes.\n",
        "\n",
        "# --- Configuración de Rutas ---\n",
        "# Creamos una carpeta dedicada en Google Drive para guardar los resultados\n",
        "PROJECT_PATH = '/content/drive/MyDrive/Kvasir_Project_Final'\n",
        "os.makedirs(PROJECT_PATH, exist_ok=True)\n",
        "print(f\"Los archivos del proyecto se guardarán en: {PROJECT_PATH}\")\n",
        "\n",
        "# --- Configuración del Dataset ---\n",
        "DATA_DIR = 'kvasir-dataset-v2'\n",
        "IMAGE_SIZE = 224\n",
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
        "\n",
        "# --- Configuración del Entrenamiento ---\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.001\n",
        "NUM_EPOCHS = 15 # Se recomienda empezar con 15-20 épocas para transfer learning\n",
        "\n",
        "# --- Configuración del Dispositivo ---\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Dispositivo de entrenamiento: {DEVICE}\")\n",
        "\n",
        "# --- Nombres de las clases (se llenarán automáticamente más adelante) ---\n",
        "CLASS_NAMES = []\n",
        "NUM_CLASSES = 0"
      ],
      "id": "0RHgc64RED80"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Adquisición y Análisis Exploratorio de Datos (EDA)\n",
        "\n",
        "En esta sección, descargamos el dataset Kvasir-v2 y realizamos un análisis inicial para entender su estructura y la distribución de las clases. Esto es crucial para confirmar si el dataset está balanceado, lo que afecta la elección de métricas y estrategias de entrenamiento."
      ],
      "metadata": {
        "id": "vVjxIG6XGa7g"
      },
      "id": "vVjxIG6XGa7g"
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# 2. ADQUISICIÓN Y ANÁLISIS EXPLORATORIO DE DATOS (EDA)\n",
        "# ===================================================================\n",
        "\n",
        "# 2.1. Descarga y Descompresión del Dataset\n",
        "# Nota: Este archivo es grande (2.3GB). La descarga puede tardar varios minutos.\n",
        "if not os.path.exists(DATA_DIR):\n",
        "    print(\"Descargando el dataset Kvasir-v2...\")\n",
        "    !wget https://datasets.simula.no/downloads/kvasir/kvasir-dataset-v2.zip -q\n",
        "    !unzip -q kvasir-dataset-v2.zip\n",
        "    print(\"Dataset descargado y descomprimido con éxito.\")\n",
        "else:\n",
        "    print(\"El dataset ya existe en el directorio.\")\n",
        "\n",
        "# 2.2. Conteo y Visualización de la Distribución de Clases\n",
        "class_counts = {}\n",
        "for class_name in os.listdir(DATA_DIR):\n",
        "    class_dir = os.path.join(DATA_DIR, class_name)\n",
        "    if os.path.isdir(class_dir):\n",
        "        class_counts[class_name] = len(os.listdir(class_dir))\n",
        "\n",
        "df_counts = pd.DataFrame(list(class_counts.items()), columns=['Clase', 'Número de Imágenes'])\n",
        "df_counts = df_counts.sort_values('Número de Imágenes', ascending=False)\n",
        "\n",
        "print(\"\\nDistribución de clases:\")\n",
        "print(df_counts)\n",
        "\n",
        "# --- Visualización ---\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x='Clase', y='Número de Imágenes', data=df_counts)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title('Distribución de Clases en Kvasir-v2')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- Conclusión del EDA ---\n",
        "print(\"\\nConclusión: El dataset está perfectamente balanceado, con 1000 imágenes por clase.\")\n",
        "print(\"Esto simplifica el entrenamiento, ya que no se requieren técnicas de balanceo complejas.\")"
      ],
      "metadata": {
        "id": "leWIaHG5Gdrh"
      },
      "execution_count": null,
      "outputs": [],
      "id": "leWIaHG5Gdrh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Preprocesamiento y Preparación de Datos\n",
        "\n",
        "Ahora, preparamos los datos para ser consumidos por los modelos de PyTorch. Los pasos clave son:\n",
        "1.  **Definir Transformaciones:** Redimensionamos las imágenes a un tamaño estándar (224x224) y las normalizamos.\n",
        "2.  **Aumento de Datos (Data Augmentation):** Aplicamos transformaciones aleatorias (volteos, rotaciones) solo al conjunto de entrenamiento para mejorar la generalización y reducir el sobreajuste.\n",
        "3.  **División de Datos:** Dividimos el dataset en conjuntos de entrenamiento (70%), validación (15%) y prueba (15%).\n",
        "4.  **Crear DataLoaders:** Envolvemos los datasets en `DataLoaders` para cargarlos eficientemente en lotes durante el entrenamiento."
      ],
      "metadata": {
        "id": "PHNq517IGjRv"
      },
      "id": "PHNq517IGjRv"
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# 3. PREPROCESAMIENTO Y PREPARACIÓN DE DATOS\n",
        "# ===================================================================\n",
        "\n",
        "# 3.1. Definición de Transformaciones\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD)\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD)\n",
        "])\n",
        "\n",
        "# 3.2. Carga y División del Dataset\n",
        "full_dataset = datasets.ImageFolder(DATA_DIR, transform=train_transforms)\n",
        "\n",
        "# Guardar nombres de clases para uso global\n",
        "CLASS_NAMES = full_dataset.classes\n",
        "NUM_CLASSES = len(CLASS_NAMES)\n",
        "print(f\"Clases encontradas ({NUM_CLASSES}): {CLASS_NAMES}\")\n",
        "\n",
        "train_size = int(0.7 * len(full_dataset))\n",
        "val_size = int(0.15 * len(full_dataset))\n",
        "test_size = len(full_dataset) - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# Aplicar las transformaciones correctas (sin aumento para validación/prueba)\n",
        "train_dataset.dataset.transform = train_transforms\n",
        "val_dataset.dataset.transform = test_transforms\n",
        "test_dataset.dataset.transform = test_transforms\n",
        "\n",
        "# 3.3. Creación de DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(f\"\\nDatos listos:\")\n",
        "print(f\"Conjunto de entrenamiento: {len(train_dataset)} imágenes en {len(train_loader)} lotes.\")\n",
        "print(f\"Conjunto de validación: {len(val_dataset)} imágenes en {len(val_loader)} lotes.\")\n",
        "print(f\"Conjunto de prueba: {len(test_dataset)} imágenes en {len(test_loader)} lotes.\")"
      ],
      "metadata": {
        "id": "HcWHdA-rGmzh"
      },
      "execution_count": null,
      "outputs": [],
      "id": "HcWHdA-rGmzh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Definición de Modelos y Bucle de Entrenamiento\n",
        "\n",
        "Esta sección contiene la lógica principal del experimento.\n",
        "1.  **Función Auxiliar:** `adapt_model_to_kvasir` adapta cualquier arquitectura pre-entrenada a nuestra tarea de 8 clases congelando las capas base y reemplazando el clasificador final.\n",
        "2.  **Función de Entrenamiento:** `train_and_evaluate` implementa el bucle de entrenamiento y validación para un modelo dado, guarda el mejor modelo basado en la precisión de validación y retorna el modelo entrenado, el historial y el tiempo de entrenamiento."
      ],
      "metadata": {
        "id": "VNZeXfi6Gp7o"
      },
      "id": "VNZeXfi6Gp7o"
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# 4. DEFINICIÓN DE MODELOS Y BUCLE DE ENTRENAMIENTO\n",
        "# ===================================================================\n",
        "\n",
        "# 4.1. Función para adaptar modelos pre-entrenados\n",
        "def adapt_model_to_kvasir(model, num_classes):\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    if hasattr(model, 'fc'): # Para ResNet\n",
        "        num_ftrs = model.fc.in_features\n",
        "        model.fc = nn.Linear(num_ftrs, num_classes)\n",
        "    elif hasattr(model, 'classifier'): # Para DenseNet y EfficientNet\n",
        "        if isinstance(model.classifier, nn.Sequential): # EfficientNet\n",
        "            num_ftrs = model.classifier[-1].in_features\n",
        "            model.classifier[-1] = nn.Linear(num_ftrs, num_classes)\n",
        "        else: # DenseNet\n",
        "            num_ftrs = model.classifier.in_features\n",
        "            model.classifier = nn.Linear(num_ftrs, num_classes)\n",
        "    return model\n",
        "\n",
        "# 4.2. Función principal de entrenamiento y evaluación\n",
        "def train_and_evaluate(model, model_name, train_loader, val_loader, num_epochs):\n",
        "    model = adapt_model_to_kvasir(model, NUM_CLASSES).to(DEVICE)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LEARNING_RATE)\n",
        "\n",
        "    best_val_accuracy = 0.0\n",
        "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "    start_time = time.time()\n",
        "\n",
        "    print(f\"--- Entrenando {model_name} ---\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Fase de Entrenamiento\n",
        "        model.train()\n",
        "        running_loss, correct_train, total_train = 0.0, 0, 0\n",
        "\n",
        "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\"):\n",
        "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_train_loss = running_loss / total_train\n",
        "        epoch_train_acc = correct_train / total_train\n",
        "        history['train_loss'].append(epoch_train_loss)\n",
        "        history['train_acc'].append(epoch_train_acc)\n",
        "\n",
        "        # Fase de Validación\n",
        "        model.eval()\n",
        "        running_val_loss, correct_val, total_val = 0.0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\"):\n",
        "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                running_val_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total_val += labels.size(0)\n",
        "                correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_val_loss = running_val_loss / total_val\n",
        "        epoch_val_acc = correct_val / total_val\n",
        "        history['val_loss'].append(epoch_val_loss)\n",
        "        history['val_acc'].append(epoch_val_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Train Loss: {epoch_train_loss:.4f} Acc: {epoch_train_acc:.4f} | Val Loss: {epoch_val_loss:.4f} Acc: {epoch_val_acc:.4f}\")\n",
        "\n",
        "        # Guardar el mejor modelo (Early Stopping implícito)\n",
        "        if epoch_val_acc > best_val_accuracy:\n",
        "            best_val_accuracy = epoch_val_acc\n",
        "            torch.save(model.state_dict(), os.path.join(PROJECT_PATH, f'{model_name}_best.pth'))\n",
        "            print(f\"Nuevo mejor modelo guardado con precisión de validación: {best_val_accuracy:.4f}\")\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    print(f\"Entrenamiento finalizado para {model_name} en {training_time:.2f} segundos.\")\n",
        "\n",
        "    # Cargar los pesos del mejor modelo guardado\n",
        "    model.load_state_dict(torch.load(os.path.join(PROJECT_PATH, f'{model_name}_best.pth')))\n",
        "\n",
        "    return model, history, training_time"
      ],
      "metadata": {
        "id": "FtJZgAhBGxgk"
      },
      "execution_count": null,
      "outputs": [],
      "id": "FtJZgAhBGxgk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Ejecución del Experimento\n",
        "\n",
        "Esta es la celda principal que ejecuta el entrenamiento para los tres modelos seleccionados. **Esta celda tomará un tiempo considerable en ejecutarse**. Una vez completada, los mejores pesos para cada modelo se guardarán en la carpeta de Google Drive definida anteriormente."
      ],
      "metadata": {
        "id": "wfeJIsDRG1L4"
      },
      "id": "wfeJIsDRG1L4"
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# 5. EJECUCIÓN DEL EXPERIMENTO\n",
        "# ===================================================================\n",
        "\n",
        "# 5.1. Inicializar modelos con pesos pre-entrenados de ImageNet\n",
        "resnet50 = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
        "efficientnet_b0 = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
        "densenet121 = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n",
        "\n",
        "# 5.2. Entrenar y evaluar cada modelo\n",
        "# NOTA: Esta parte tomará un tiempo considerable.\n",
        "trained_resnet, history_resnet, time_resnet = train_and_evaluate(resnet50, \"ResNet50\", train_loader, val_loader, NUM_EPOCHS)\n",
        "trained_efficientnet, history_efficientnet, time_efficientnet = train_and_evaluate(efficientnet_b0, \"EfficientNet-B0\", train_loader, val_loader, NUM_EPOCHS)\n",
        "trained_densenet, history_densenet, time_densenet = train_and_evaluate(densenet121, \"DenseNet121\", train_loader, val_loader, NUM_EPOCHS)\n",
        "\n",
        "# 5.3. Guardar historiales de entrenamiento para análisis futuro\n",
        "histories = {\n",
        "    \"ResNet50\": history_resnet,\n",
        "    \"EfficientNet-B0\": history_efficientnet,\n",
        "    \"DenseNet121\": history_densenet\n",
        "}\n",
        "with open(os.path.join(PROJECT_PATH, 'training_histories.pkl'), 'wb') as f:\n",
        "    pickle.dump(histories, f)\n",
        "print(\"\\n¡Todos los modelos han sido entrenados y los historiales guardados!\")\n",
        "\n",
        "# 5.4. Recopilar, mostrar y guardar datos de costo computacional\n",
        "# Esta tabla se guardará por separado para el análisis.\n",
        "def count_total_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "cost_data = {\n",
        "    \"ResNet50\": {\n",
        "        \"Training Time (s)\": time_resnet,\n",
        "        \"Total Parameters\": count_total_parameters(models.resnet50())\n",
        "    },\n",
        "    \"EfficientNet-B0\": {\n",
        "        \"Training Time (s)\": time_efficientnet,\n",
        "        \"Total Parameters\": count_total_parameters(models.efficientnet_b0())\n",
        "    },\n",
        "    \"DenseNet121\": {\n",
        "        \"Training Time (s)\": time_densenet,\n",
        "        \"Total Parameters\": count_total_parameters(models.densenet121())\n",
        "    }\n",
        "}\n",
        "cost_df = pd.DataFrame(cost_data).T\n",
        "\n",
        "# Formatear para mejor visualización\n",
        "cost_df_display = cost_df.copy()\n",
        "cost_df_display['Total Parameters'] = cost_df_display['Total Parameters'].apply(lambda x: f\"{x/1e6:.2f}M\")\n",
        "cost_df_display['Training Time (s)'] = cost_df_display['Training Time (s)'].apply(lambda x: f\"{x:.2f}\")\n",
        "\n",
        "print(\"\\n--- Tabla 2: Comparación de Costo Computacional ---\")\n",
        "print(cost_df_display)\n",
        "\n",
        "# Guardar los datos SIN formatear para facilitar su re-uso numérico.\n",
        "cost_df.to_csv(os.path.join(PROJECT_PATH, 'computational_cost.csv'))\n",
        "print(\"\\nDatos de costo computacional guardados en Drive.\")"
      ],
      "metadata": {
        "id": "oVBh5ryWG2bp"
      },
      "execution_count": null,
      "outputs": [],
      "id": "oVBh5ryWG2bp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Análisis Comparativo de Resultados\n",
        "\n",
        "En esta sección, cargamos los modelos guardados (o usamos los que acabamos de entrenar) y los evaluamos en el conjunto de prueba. Comparamos su rendimiento en términos de precisión, costo computacional y visualizamos sus curvas de aprendizaje para detectar sobreajuste.\n",
        "\n",
        "**NOTA:** Si te reconectas al notebook y quieres saltarte el re-entrenamiento, puedes ejecutar las celdas de la sección 1 a la 4, y luego ejecutar directamente la **Celda 6.1 (Carga de Modelos)** antes de continuar con el resto de esta sección."
      ],
      "metadata": {
        "id": "CG1YrG80G6yU"
      },
      "id": "CG1YrG80G6yU"
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# 6. ANÁLISIS COMPARATIVO DE RESULTADOS\n",
        "# ===================================================================\n",
        "\n",
        "# 6.1. Carga de Modelos Entrenados desde Drive\n",
        "# Este paso permite saltar el re-entrenamiento en futuras sesiones.\n",
        "\n",
        "# --- Crear arquitecturas base ---\n",
        "resnet50_loaded = models.resnet50()\n",
        "efficientnet_b0_loaded = models.efficientnet_b0()\n",
        "densenet121_loaded = models.densenet121()\n",
        "\n",
        "# --- Adaptarlas a nuestras 8 clases ---\n",
        "resnet50_loaded = adapt_model_to_kvasir(resnet50_loaded, NUM_CLASSES)\n",
        "efficientnet_b0_loaded = adapt_model_to_kvasir(efficientnet_b0_loaded, NUM_CLASSES)\n",
        "densenet121_loaded = adapt_model_to_kvasir(densenet121_loaded, NUM_CLASSES)\n",
        "\n",
        "# --- Cargar los pesos (state_dict) guardados ---\n",
        "try:\n",
        "    print(\"Cargando pesos de los mejores modelos desde Google Drive...\")\n",
        "    resnet50_loaded.load_state_dict(torch.load(os.path.join(PROJECT_PATH, 'ResNet50_best.pth')))\n",
        "    efficientnet_b0_loaded.load_state_dict(torch.load(os.path.join(PROJECT_PATH, 'EfficientNet-B0_best.pth')))\n",
        "    densenet121_loaded.load_state_dict(torch.load(os.path.join(PROJECT_PATH, 'DenseNet121_best.pth')))\n",
        "    print(\"Pesos cargados con éxito.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"¡ERROR! No se encontraron los archivos .pth de los modelos.\")\n",
        "    raise\n",
        "\n",
        "# --- Mover a GPU y poner en modo evaluación ---\n",
        "trained_models = {\n",
        "    \"ResNet50\": resnet50_loaded.to(DEVICE).eval(),\n",
        "    \"EfficientNet-B0\": efficientnet_b0_loaded.to(DEVICE).eval(),\n",
        "    \"DenseNet121\": densenet121_loaded.to(DEVICE).eval()\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "id": "aqML9iSlHCX2"
      },
      "execution_count": null,
      "outputs": [],
      "id": "aqML9iSlHCX2"
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.2. Generación de la Tabla de Rendimiento en el Conjunto de Prueba\n",
        "performance_data = {}\n",
        "\n",
        "for name, model in trained_models.items():\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "    performance_data[name] = {\n",
        "        'Test Accuracy': f\"{accuracy:.4f}\",\n",
        "        'Test F1-Score (weighted)': f\"{f1:.4f}\",\n",
        "    }\n",
        "\n",
        "performance_df = pd.DataFrame(performance_data).T\n",
        "\n",
        "print(\"\\n--- Tabla 1: Resultados de Clasificación en el Conjunto de Prueba ---\")\n",
        "print(performance_df)\n",
        "\n",
        "# Guardar la tabla en Drive\n",
        "performance_df.to_csv(os.path.join(PROJECT_PATH, 'performance_results.csv'))"
      ],
      "metadata": {
        "id": "02ZqNVaCHInP"
      },
      "execution_count": null,
      "outputs": [],
      "id": "02ZqNVaCHInP"
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.3. Generación de la Tabla de Costo Computacional\n",
        "try:\n",
        "    cost_df = pd.read_csv(os.path.join(PROJECT_PATH, 'computational_cost.csv'), index_col=0)\n",
        "\n",
        "    # Formatear para mejor visualización\n",
        "    cost_df['Total Parameters'] = cost_df['Total Parameters'].apply(lambda x: f\"{x/1e6:.2f}M\")\n",
        "    cost_df['Training Time (s)'] = cost_df['Training Time (s)'].apply(lambda x: f\"{x:.2f}\")\n",
        "\n",
        "    print(\"\\n--- Tabla 2: Comparación de Costo Computacional ---\")\n",
        "    print(cost_df)\n",
        "except FileNotFoundError:\n",
        "    print(\"\\n¡ERROR! Archivo 'computational_cost.csv' no encontrado. Ejecuta la celda de entrenamiento (Sección 5) primero.\")"
      ],
      "metadata": {
        "id": "fgD29qfR92IQ"
      },
      "execution_count": null,
      "outputs": [],
      "id": "fgD29qfR92IQ"
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.4. Visualización de Curvas de Aprendizaje\n",
        "\n",
        "# Cargar los historiales guardados\n",
        "with open(os.path.join(PROJECT_PATH, 'training_histories.pkl'), 'rb') as f:\n",
        "    histories = pickle.load(f)\n",
        "\n",
        "def plot_history(history, model_name):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history['train_acc'], label='Train Accuracy')\n",
        "    plt.plot(history['val_acc'], label='Validation Accuracy')\n",
        "    plt.title(f'{model_name} - Curvas de Precisión')\n",
        "    plt.xlabel(\"Épocas\")\n",
        "    plt.ylabel(\"Precisión\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history['train_loss'], label='Train Loss')\n",
        "    plt.plot(history['val_loss'], label='Validation Loss')\n",
        "    plt.title(f'{model_name} - Curvas de Pérdida')\n",
        "    plt.xlabel(\"Épocas\")\n",
        "    plt.ylabel(\"Pérdida\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "for name, history in histories.items():\n",
        "    plot_history(history, name)"
      ],
      "metadata": {
        "id": "s__BLpy3HMsH"
      },
      "execution_count": null,
      "outputs": [],
      "id": "s__BLpy3HMsH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.5. Análisis de Interpretabilidad con Grad-CAM\n",
        "\n",
        "Para entender en qué regiones de la imagen se están enfocando los modelos para tomar sus decisiones, utilizamos Grad-CAM (Gradient-weighted Class Activation Mapping). Esto es especialmente valioso en el dominio médico para verificar que el modelo se fija en las patologías correctas."
      ],
      "metadata": {
        "id": "ozB1iPzRHP9k"
      },
      "id": "ozB1iPzRHP9k"
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# 6.5. ANÁLISIS DE INTERPRETABILIDAD EXTENDIDO (GRAD-CAM)\n",
        "# ===================================================================\n",
        "\n",
        "# 6.5.1. Instalación e importaciones\n",
        "!pip install -q grad-cam\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "\n",
        "# 6.5.2. Función para visualizar Grad-CAM\n",
        "def visualize_grad_cam(model, model_name, target_layers, num_examples=3):\n",
        "    print(f\"\\n--- Visualizando Grad-CAM para {model_name} ---\")\n",
        "\n",
        "    # Descongelar el modelo para el backward pass\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = True\n",
        "    model.eval()\n",
        "\n",
        "    # Obtener un lote de datos\n",
        "    data_iter = iter(test_loader)\n",
        "\n",
        "    for i in range(num_examples):\n",
        "        images, labels = next(data_iter)\n",
        "\n",
        "        # Seleccionar una imagen y su etiqueta\n",
        "        input_tensor = images[i].unsqueeze(0).to(DEVICE)\n",
        "        label_index = labels[i].item()\n",
        "        true_class_name = CLASS_NAMES[label_index]\n",
        "\n",
        "        # Predecir la clase\n",
        "        output = model(input_tensor)\n",
        "        _, pred_index_tensor = torch.max(output, 1)\n",
        "        pred_index = pred_index_tensor.item()\n",
        "        pred_class_name = CLASS_NAMES[pred_index]\n",
        "\n",
        "        # Preparar imagen para visualización (des-normalizar)\n",
        "        image_for_display = images[i].numpy().transpose((1, 2, 0))\n",
        "        image_for_display = IMAGENET_STD * image_for_display + IMAGENET_MEAN\n",
        "        image_for_display = np.clip(image_for_display, 0, 1)\n",
        "\n",
        "        # Ejecutar Grad-CAM\n",
        "        cam = GradCAM(model=model, target_layers=target_layers)\n",
        "        targets = [ClassifierOutputTarget(pred_index)]\n",
        "        grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
        "        grayscale_cam = grayscale_cam[0, :]\n",
        "        visualization = show_cam_on_image(image_for_display, grayscale_cam, use_rgb=True, image_weight=0.6)\n",
        "\n",
        "        # Mostrar resultados\n",
        "        fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
        "        fig.suptitle(f\"Ejemplo {i+1}: Real = '{true_class_name}' | Predicción = '{pred_class_name}'\", fontsize=14)\n",
        "        axs[0].imshow(image_for_display)\n",
        "        axs[0].set_title(\"Imagen Original\")\n",
        "        axs[0].axis('off')\n",
        "        axs[1].imshow(visualization)\n",
        "        axs[1].set_title(\"Grad-CAM\")\n",
        "        axs[1].axis('off')\n",
        "        plt.show()\n",
        "\n",
        "# 6.5.3. Ejecutar visualizaciones para el mejor modelo (EfficientNet-B0)\n",
        "best_model_name = 'EfficientNet-B0'\n",
        "best_model = trained_models[best_model_name]\n",
        "target_layers_efficientnet = [best_model.features[-1]] # Capa final de características de EfficientNet\n",
        "visualize_grad_cam(best_model, best_model_name, target_layers_efficientnet, num_examples=4)"
      ],
      "metadata": {
        "id": "aG3ADXnjHR-m"
      },
      "execution_count": null,
      "outputs": [],
      "id": "aG3ADXnjHR-m"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Conclusiones y Trabajo Futuro\n",
        "\n",
        "### Conclusiones\n",
        "*   **Rendimiento:** El modelo [ResNet50] alcanzó el mejor rendimiento en el conjunto de prueba con una precisión de [86.83] y un F1-Score de [86.77%].\n",
        "*   **Trade-off:** Sin embargo tiene [más] parámetros [25.56M], contra [EfficientNet-B0] que solo tiene [5.29M] con esto demostramos que [EfficientNet-B0] tiene un excelente balance entre precisión y eficiencia computacional.\n",
        "*   **Interpretabilidad:** Las visualizaciones de Grad-CAM confirman que el modelo se enfoca en las regiones patológicas relevantes, aumentando la confianza en sus predicciones.\n",
        "\n",
        "### Trabajo Futuro\n",
        "*   **Fine-Tuning:** Descongelar capas adicionales del modelo base para un ajuste fino podría mejorar aún más la precisión.\n",
        "*   **Nuevas Arquitecturas:** Explorar el rendimiento de Vision Transformers (ViT) o arquitecturas híbridas en este dataset.\n",
        "*   **Tareas Adicionales:** Aprovechar las máscaras de segmentación del dataset Kvasir-SEG para entrenar modelos de segmentación semántica de pólipos."
      ],
      "metadata": {
        "id": "McePN4InHU1u"
      },
      "id": "McePN4InHU1u"
    }
  ]
}